{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2aef2ab6",
   "metadata": {},
   "source": [
    "# Classification (LSTM, RNN, GRU, 1D-CNN) on Concatenated Multivariate Time Series Data (2D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea62f006",
   "metadata": {},
   "source": [
    "### Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78559b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# float(\"{:.2f}\".format(13.949999999999999))\n",
    "\n",
    "def TSS(TP,TN,FP,FN):\n",
    "    TSS_value = (TP / (TP + FN)) - (FP / (FP + TN))\n",
    "    return TSS_value\n",
    "\n",
    "def HSS1(TP,TN,FP,FN):\n",
    "    HSS1_value = (2 * (TP * TN - FP * FN)) / ((TP + FN) * (FN + TN) + (TP + FP) * (FP + TN))\n",
    "    return HSS1_value\n",
    "    \n",
    "def HSS2(TP,TN,FP,FN):\n",
    "    HSS2_value = (2 * (TP * TN - FP * FN)) / ((TP + FP) * (FN + TN) + (TP + FN) * (FP + TN))\n",
    "    return HSS2_value\n",
    "\n",
    "def GSS(TP,TN,FP,FN):\n",
    "    GSS_value = (TP - (TP + FP) * (TP + FN) / (TP + FP + FN + TN))\n",
    "    return GSS_value\n",
    "\n",
    "def Recall(TP,TN,FP,FN):\n",
    "    Recall_value = (TP) / (TP + FN)\n",
    "    return Recall_value\n",
    "\n",
    "def FPR(TP,TN,FP,FN):\n",
    "    fpr_value = (FP) / (FP + TN)\n",
    "    return fpr_value\n",
    "\n",
    "def Accuracy(TP,TN,FP,FN):\n",
    "    accuracy_value = (TP + TN) / (TP + TN + FP + FN)\n",
    "    return accuracy_value\n",
    "\n",
    "def Precision(TP,TN,FP,FN):\n",
    "    precision_value = (TP) / (TP + FP)\n",
    "    return precision_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342a8fa7",
   "metadata": {},
   "source": [
    "# Loading the Final Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f52133",
   "metadata": {},
   "source": [
    "### Reading the Data built by the Previous Notebooks (LSBZM Normalization and FPCKNN Imputation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bac9402a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P1 Nan-Value: False\n",
      "(73492, 1440)\n",
      "P2 Nan-Value: False\n",
      "(88557, 1440)\n",
      "P3 Nan-Value: False\n",
      "(42510, 1440)\n",
      "P4 Nan-Value: False\n",
      "(51261, 1440)\n",
      "P5 Nan-Value: False\n",
      "(75365, 1440)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "data_dir = \"/Users/samskanderi/Documents/Research_Project/SWANSF/code/5_4_FinalData_Concatenation_LSBZM_KnnImputation/\"\n",
    "X_train_NewF_LSBZM = []\n",
    "Y_train_NewF_LSBZM = []\n",
    "\n",
    "\n",
    "num_partitions = 5\n",
    "\n",
    "for i in range(0,num_partitions):\n",
    "    with open(data_dir + \"Partition\" + str(i+1) + \"_Concatenation_LSBZM_KnnImputation\" + \".pkl\", 'rb') as f:\n",
    "        X_train_NewF_LSBZM.append(pickle.load(f))\n",
    "    print(\"P\"+str(i+1)+\" Nan-Value: \"+ str(np.isnan(X_train_NewF_LSBZM[i]).any() or np.isinf(X_train_NewF_LSBZM[i]).any()))\n",
    "    \n",
    "    with open(data_dir + \"Partition\" + str(i+1) + \"_Labels_Concatenation_LSBZM_KnnImputation\" + \".pkl\", 'rb') as f:\n",
    "        Y_train_NewF_LSBZM.append(pickle.load(f))\n",
    "    print(X_train_NewF_LSBZM[i].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ac39b3",
   "metadata": {},
   "source": [
    "### Reading the Data built by the Previous Notebooks (MinMax Normalization and Mean Imputation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d42d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "data_dir = \"/Users/samskanderi/Documents/Research_Project/SWANSF/code/5_0_FinalData_ZM_BaseLineImputation/\"\n",
    "X_train_NewF_M = []\n",
    "Y_train_NewF_M = []\n",
    "\n",
    "\n",
    "num_partitions = 5\n",
    "\n",
    "for i in range(0,num_partitions):\n",
    "    with open(data_dir + \"Partition\" + str(i+1) + \"_Concatenation_MinMaxNorm_MeanImputation\" + \".pkl\", 'rb') as f:\n",
    "        X_train_NewF_M.append(pickle.load(f))\n",
    "    print(\"P\"+str(i+1)+\" Nan-Value: \"+ str(np.isnan(X_train_NewF_M[i]).any() or np.isinf(X_train_NewF_M[i]).any()))\n",
    "    \n",
    "    with open(data_dir + \"Partition\" + str(i+1) + \"_Labels_Concatenation_MinMaxNorm_MeanImputation\" + \".pkl\", 'rb') as f:\n",
    "        Y_train_NewF_M.append(pickle.load(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98d96a5",
   "metadata": {},
   "source": [
    "### Reading the Data built by the Previous Notebooks (Z-score Normalization and Mean Imputation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d15d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "data_dir = \"/Users/samskanderi/Documents/Research_Project/SWANSF/code/5_0_FinalData_ZM_BaseLineImputation/\"\n",
    "X_train_NewF_Z = []\n",
    "Y_train_NewF_Z = []\n",
    "\n",
    "\n",
    "num_partitions = 5\n",
    "\n",
    "for i in range(0,num_partitions):\n",
    "    with open(data_dir + \"Partition\" + str(i+1) + \"_Concatenation_ZNorm_MeanImputation\" + \".pkl\", 'rb') as f:\n",
    "        X_train_NewF_Z.append(pickle.load(f))\n",
    "    print(\"P\"+str(i+1)+\" Nan-Value: \"+ str(np.isnan(X_train_NewF_Z[i]).any() or np.isinf(X_train_NewF_Z[i]).any()))\n",
    "    \n",
    "    with open(data_dir + \"Partition\" + str(i+1) + \"_Labels_Concatenation_ZNorm_MeanImputation\" + \".pkl\", 'rb') as f:\n",
    "        Y_train_NewF_Z.append(pickle.load(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0771e719",
   "metadata": {},
   "source": [
    "### Reading the Data built by the Previous Notebooks (MinMax Normalization and Nextvalue Imputation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82298ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "data_dir = \"/Users/samskanderi/Documents/Research_Project/SWANSF/code/5_0_FinalData_ZM_BaseLineImputation/\"\n",
    "X_train_NewF_M_N = []\n",
    "Y_train_NewF_M_N = []\n",
    "\n",
    "\n",
    "num_partitions = 5\n",
    "\n",
    "for i in range(0,num_partitions):\n",
    "    with open(data_dir + \"Partition\" + str(i+1) + \"_Concatenation_MinMaxNorm_NextvalueImputation\" + \".pkl\", 'rb') as f:\n",
    "        X_train_NewF_M_N.append(pickle.load(f))\n",
    "    print(\"P\"+str(i+1)+\" Nan-Value: \"+ str(np.isnan(X_train_NewF_M_N[i]).any() or np.isinf(X_train_NewF_M_N[i]).any()))\n",
    "    \n",
    "    with open(data_dir + \"Partition\" + str(i+1) + \"_Labels_Concatenation_MinMaxNorm_NextvalueImputation\" + \".pkl\", 'rb') as f:\n",
    "        Y_train_NewF_M_N.append(pickle.load(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e2208c",
   "metadata": {},
   "source": [
    "### Reading the Data built by the Previous Notebooks (Z-score Normalization and Nextvalue Imputation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533a22f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "data_dir = \"/Users/samskanderi/Documents/Research_Project/SWANSF/code/5_0_FinalData_ZM_BaseLineImputation/\"\n",
    "X_train_NewF_Z_N = []\n",
    "Y_train_NewF_Z_N = []\n",
    "\n",
    "\n",
    "num_partitions = 5\n",
    "\n",
    "for i in range(0,num_partitions):\n",
    "    with open(data_dir + \"Partition\" + str(i+1) + \"_Concatenation_ZNorm_NextvalueImputation\" + \".pkl\", 'rb') as f:\n",
    "        X_train_NewF_Z_N.append(pickle.load(f))\n",
    "    print(\"P\"+str(i+1)+\" Nan-Value: \"+ str(np.isnan(X_train_NewF_Z_N[i]).any() or np.isinf(X_train_NewF_Z_N[i]).any()))\n",
    "    \n",
    "    with open(data_dir + \"Partition\" + str(i+1) + \"_Labels_Concatenation_ZNorm_NextvalueImputation\" + \".pkl\", 'rb') as f:\n",
    "        Y_train_NewF_Z_N.append(pickle.load(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e879937b",
   "metadata": {},
   "source": [
    "### Function to run the classification on different Train-Test Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69a74f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_training(name, X_train, Y_train, training_func, num, rocket_kernels= 1500, tsf_estimator=25):\n",
    "    #kfold = np.array([[1,2],[1,3],[1,4],[1,5],[2,3],[2,4],[2,5],[3,4],[3,5],[4,5]])\n",
    "    kfold = np.array([[1,2],[2,3],[3,4],[4,5]])\n",
    "\n",
    "    metrics = []\n",
    "    metrics_values = np.array([])\n",
    "    \n",
    "    for i in range(0, num):\n",
    "        train_index = kfold[i,0]\n",
    "        test_index = kfold[i,1]\n",
    "        metrics_values = training_func(X_train[train_index-1], Y_train[train_index-1], X_train[test_index-1], Y_train[test_index-1])\n",
    "        while (metrics_values[0]==0):\n",
    "            metrics_values = training_func(X_train[train_index-1], Y_train[train_index-1], X_train[test_index-1], Y_train[test_index-1])\n",
    "        \n",
    "        metrics.append(np.append(np.append(train_index, test_index), metrics_values))\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a024ab",
   "metadata": {},
   "source": [
    "## Code of the Classification techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13583cb",
   "metadata": {},
   "source": [
    "### Rocket (RidgeClassifierCV) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a7ff942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROCKET with RidgeClassifierCV\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "from sktime.transformations.panel.rocket import Rocket\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import joblib\n",
    "from sktime.datatypes._panel._convert import from_2d_array_to_nested\n",
    "\n",
    "def rocket_model(X_train, Y_train, X_test, Y_test, rocket_kernels=1500):\n",
    "\n",
    "    data_dir = \"/Users/samskanderi/Documents/Research_Project/SWANSF/code/models/rocket/\"\n",
    "    \n",
    "    X_train = from_2d_array_to_nested(X_train)\n",
    "    X_test = from_2d_array_to_nested(X_test)\n",
    "\n",
    "    rocket = Rocket(num_kernels=rocket_kernels)\n",
    "    rocket.fit(X_train)\n",
    "    X_train_transform = rocket.transform(X_train)\n",
    "        \n",
    "    classifier = RidgeClassifierCV()\n",
    "    classifier.fit(X_train_transform, Y_train)\n",
    "    \n",
    "    X_test_transform = rocket.transform(X_test)\n",
    "    \n",
    "    y_pred = classifier.predict(X_test_transform)\n",
    "    \n",
    "    print(str(X_train.shape)+': Rocket Classifier is Done! \\n')\n",
    "\n",
    "    confusion = confusion_matrix(Y_test, y_pred)\n",
    "    tn, fp, fn, tp = confusion.ravel()\n",
    "\n",
    "    tss = TSS(tp,tn,fp,fn)\n",
    "    hss1 = HSS1(tp,tn,fp,fn)\n",
    "    hss2 = HSS2(tp,tn,fp,fn)\n",
    "    gss = GSS(tp,tn,fp,fn)\n",
    "    recall = Recall(tp,tn,fp,fn)\n",
    "    precision = Precision(tp,tn,fp,fn)\n",
    "    \n",
    "    output_values = np.array([tp, fn, fp, tn, tss, hss1, hss2, gss, recall, precision])\n",
    "\n",
    "\n",
    "    #joblib.dump(classifier, data_dir + \"rocket_model.pkl\")\n",
    "\n",
    "    #loaded_rocket_model = joblib.load(data_dir + \"rocket_model_sgd.pkl\")\n",
    "    \n",
    "    return output_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8840de35",
   "metadata": {},
   "source": [
    "### TimeSeriesForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06c0fef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TimeSeriesForest\n",
    "\n",
    "from sktime.classification.interval_based import TimeSeriesForestClassifier\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import joblib\n",
    "\n",
    "def tsf_model(X_train, Y_train, X_test, Y_test, tsf_estimator=60):\n",
    "\n",
    "    data_dir = \"/Users/samskanderi/Documents/Research_Project/SWANSF/code/models/TSF/\"\n",
    "    \n",
    "    \n",
    "    tsf_classifier = TimeSeriesForestClassifier(n_estimators=tsf_estimator)\n",
    "    tsf_classifier.fit(X_train, Y_train)\n",
    "    y_pred = tsf_classifier.predict(X_test)\n",
    "    \n",
    "    print(str(X_train.shape)+': TSF Classifier is Done! \\n')\n",
    "    \n",
    "\n",
    "    confusion = confusion_matrix(Y_test, y_pred)\n",
    "    tn, fp, fn, tp = confusion.ravel()\n",
    "\n",
    "    tss = TSS(tp,tn,fp,fn)\n",
    "    hss1 = HSS1(tp,tn,fp,fn)\n",
    "    hss2 = HSS2(tp,tn,fp,fn)\n",
    "    gss = GSS(tp,tn,fp,fn)\n",
    "    recall = Recall(tp,tn,fp,fn)\n",
    "    precision = Precision(tp,tn,fp,fn)\n",
    "    \n",
    "    output_values = np.array([tp, fn, fp, tn, tss, hss1, hss2, gss, recall, precision])\n",
    "\n",
    "\n",
    "    #joblib.dump(classifier, data_dir + \"tsf_model.pkl\")\n",
    "\n",
    "    #loaded_rocket_model = joblib.load(data_dir + \"tsf_model.pkl\")\n",
    "    \n",
    "    return output_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18eb4d25",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92a02e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def lstm_model(X_train, Y_train, X_test, Y_test):\n",
    "    \n",
    "    data_dir = \"/Users/samskanderi/Documents/Research_Project/SWANSF/code/models/\"\n",
    "    \n",
    "    verbose, epochs, batch_size = 0, 15, 32\n",
    "    n_timesteps, n_features = 60, 24\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(120, input_shape=(n_timesteps,n_features)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(120, activation='relu'))\n",
    "    model.add(Dense(2, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=keras.metrics.SpecificityAtSensitivity(sensitivity=0.95))\n",
    "    \n",
    "    model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "    \n",
    "    best_threshold = 0.0\n",
    "    best_tss = 0.0\n",
    "    y_pred = model.predict(X_test)\n",
    "    # evaluate model\n",
    "    for i in range(1, 100):\n",
    "\n",
    "        threshold = i / 100 # Adjust the threshold as needed\n",
    "        y_pred_binary = (y_pred > threshold).astype(int)\n",
    "        confusion = confusion_matrix(Y_test, y_pred_binary)\n",
    "        tn, fp, fn, tp = confusion.ravel()\n",
    "        tss = TSS(tp,tn,fp,fn)\n",
    "        if tss > best_tss:\n",
    "            best_tss = tss\n",
    "            best_threshold = i / 100\n",
    "        \n",
    "    \n",
    "    print(str(X_train.shape)+': LSTM Classifier is Done! \\n')\n",
    "\n",
    "\n",
    "    threshold = best_threshold # Adjust the threshold as needed\n",
    "    y_pred_binary = (y_pred > threshold).astype(int)\n",
    "    confusion = confusion_matrix(Y_test, y_pred_binary)\n",
    "    tn, fp, fn, tp = confusion.ravel()\n",
    "\n",
    "    tss = TSS(tp,tn,fp,fn)\n",
    "    hss1 = HSS1(tp,tn,fp,fn)\n",
    "    hss2 = HSS2(tp,tn,fp,fn)\n",
    "    gss = GSS(tp,tn,fp,fn)\n",
    "    recall = Recall(tp,tn,fp,fn)\n",
    "    precision = Precision(tp,tn,fp,fn)\n",
    "    \n",
    "    output_values = np.array([tp, fn, fp, tn, tss, hss1, hss2, gss, recall, precision])\n",
    "\n",
    "\n",
    "    #joblib.dump(classifier, data_dir + \"mlp_model.pkl\")\n",
    "\n",
    "    #loaded_mlp_model = joblib.load(data_dir + \"mlp_model.pkl\")\n",
    "    \n",
    "    return output_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694e693a",
   "metadata": {},
   "source": [
    "### 1D-CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "265014e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def cnn_model(X_train, Y_train, X_test, Y_test):\n",
    "    \n",
    "    data_dir = \"/Users/samskanderi/Documents/Research_Project/SWANSF/code/models/\"\n",
    "    \n",
    "    verbose, epochs, batch_size = 0, 15, 32\n",
    "    n_timesteps, n_features = 60, 24\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=64, kernel_size=5, activation='relu', input_shape=(n_timesteps,n_features)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(357, activation='relu'))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(2, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=keras.metrics.SpecificityAtSensitivity(sensitivity=0.95))\n",
    "    \n",
    "    model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "    \n",
    "    # evaluate model\n",
    "    y_pred = model.predict(X_test)\n",
    "    threshold = 0.05  # Adjust the threshold as needed\n",
    "    y_pred_binary = (y_pred > threshold).astype(int)\n",
    "    \n",
    "    print(str(X_train.shape)+': 1D-CNN Classifier is Done! \\n')\n",
    "\n",
    "\n",
    "    confusion = confusion_matrix(Y_test, y_pred_binary)\n",
    "    tn, fp, fn, tp = confusion.ravel()\n",
    "\n",
    "    tss = TSS(tp,tn,fp,fn)\n",
    "    hss1 = HSS1(tp,tn,fp,fn)\n",
    "    hss2 = HSS2(tp,tn,fp,fn)\n",
    "    gss = GSS(tp,tn,fp,fn)\n",
    "    recall = Recall(tp,tn,fp,fn)\n",
    "    precision = Precision(tp,tn,fp,fn)\n",
    "    \n",
    "    output_values = np.array([tp, fn, fp, tn, tss, hss1, hss2, gss, recall, precision])\n",
    "\n",
    "\n",
    "    #joblib.dump(classifier, data_dir + \"mlp_model.pkl\")\n",
    "\n",
    "    #loaded_mlp_model = joblib.load(data_dir + \"mlp_model.pkl\")\n",
    "    \n",
    "    return output_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026a3469",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acb129af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import SimpleRNN, Dense, Dropout\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def rnn_model(X_train, Y_train, X_test, Y_test):\n",
    "    \n",
    "    data_dir = \"/Users/samskanderi/Documents/Research_Project/SWANSF/code/models/\"\n",
    "    \n",
    "    verbose, epochs, batch_size = 0, 15, 32\n",
    "    n_timesteps, n_features = 60, 24\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(SimpleRNN(units=120, activation='relu', input_shape=(n_timesteps,n_features)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(120, activation='relu'))\n",
    "    model.add(Dense(2, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=keras.metrics.SpecificityAtSensitivity(sensitivity=0.95))\n",
    "\n",
    "    model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "    \n",
    "    # evaluate model\n",
    "    y_pred = model.predict(X_test)\n",
    "    threshold = 0.8  # Adjust the threshold as needed\n",
    "    y_pred_binary = (y_pred > threshold).astype(int)\n",
    "    \n",
    "    print(str(X_train.shape)+': RNN Classifier is Done! \\n')\n",
    "\n",
    "\n",
    "    confusion = confusion_matrix(Y_test, y_pred_binary)\n",
    "    tn, fp, fn, tp = confusion.ravel()\n",
    "\n",
    "    tss = TSS(tp,tn,fp,fn)\n",
    "    hss1 = HSS1(tp,tn,fp,fn)\n",
    "    hss2 = HSS2(tp,tn,fp,fn)\n",
    "    gss = GSS(tp,tn,fp,fn)\n",
    "    recall = Recall(tp,tn,fp,fn)\n",
    "    precision = Precision(tp,tn,fp,fn)\n",
    "    \n",
    "    output_values = np.array([tp, fn, fp, tn, tss, hss1, hss2, gss, recall, precision])\n",
    "\n",
    "\n",
    "    #joblib.dump(classifier, data_dir + \"mlp_model.pkl\")\n",
    "\n",
    "    #loaded_mlp_model = joblib.load(data_dir + \"mlp_model.pkl\")\n",
    "    \n",
    "    return output_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a149591",
   "metadata": {},
   "source": [
    "### GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8b000d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import GRU, Dense, Dropout\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def gru_model(X_train, Y_train, X_test, Y_test):\n",
    "    \n",
    "    data_dir = \"/Users/samskanderi/Documents/Research_Project/SWANSF/code/models/\"\n",
    "    \n",
    "    verbose, epochs, batch_size = 0, 15, 32\n",
    "    n_timesteps, n_features = 60, 24\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(GRU(units=120, activation='relu', input_shape=(n_timesteps,n_features)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(120, activation='relu'))\n",
    "    model.add(Dense(2, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=keras.metrics.SpecificityAtSensitivity(sensitivity=0.95))\n",
    "\n",
    "    model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "    \n",
    "    # evaluate model\n",
    "    y_pred = model.predict(X_test)\n",
    "    threshold = 0.5  # Adjust the threshold as needed\n",
    "    y_pred_binary = (y_pred > threshold).astype(int)\n",
    "    \n",
    "    print(str(X_train.shape)+': GRU Classifier is Done! \\n')\n",
    "\n",
    "\n",
    "    confusion = confusion_matrix(Y_test, y_pred_binary)\n",
    "    tn, fp, fn, tp = confusion.ravel()\n",
    "\n",
    "    tss = TSS(tp,tn,fp,fn)\n",
    "    hss1 = HSS1(tp,tn,fp,fn)\n",
    "    hss2 = HSS2(tp,tn,fp,fn)\n",
    "    gss = GSS(tp,tn,fp,fn)\n",
    "    recall = Recall(tp,tn,fp,fn)\n",
    "    precision = Precision(tp,tn,fp,fn)\n",
    "    \n",
    "    output_values = np.array([tp, fn, fp, tn, tss, hss1, hss2, gss, recall, precision])\n",
    "\n",
    "\n",
    "    #joblib.dump(classifier, data_dir + \"mlp_model.pkl\")\n",
    "\n",
    "    #loaded_mlp_model = joblib.load(data_dir + \"mlp_model.pkl\")\n",
    "    \n",
    "    return output_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09b2613",
   "metadata": {},
   "source": [
    "### Saving the Result of Classification on File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2fae2c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(reslut, name):\n",
    "    data_dir = \"/Users/samskanderi/Documents/Research_Project/SWANSF/code/results/\"\n",
    "\n",
    "    with open(data_dir + name + \".pkl\", 'wb') as f:\n",
    "        pickle.dump(reslut, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b641ae",
   "metadata": {},
   "source": [
    "## Running Experiments "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90d5e67",
   "metadata": {},
   "source": [
    "## 2D to 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba8c56e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def the_2dto3d(X_train):\n",
    "    num_attributes = 24\n",
    "    num_partitions = 5\n",
    "    num_timestamps = 60\n",
    "    X_train_concat_ZM_3D = []\n",
    "    for i in range(0, num_partitions):\n",
    "        new_3D = np.zeros((X_train[i].shape[0], num_timestamps, num_attributes))\n",
    "\n",
    "        for j in range(0, X_train[i].shape[0]):\n",
    "            for m in range(0, num_attributes):\n",
    "                new_3D[j,:,m] = X_train[i][j,m*num_timestamps:(m+1)*num_timestamps]\n",
    "        X_train_concat_ZM_3D.append(new_3D)\n",
    "    return X_train_concat_ZM_3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1fae5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_concat_LSBZM_3D = the_2dto3d(X_train_NewF_LSBZM)\n",
    "del X_train_NewF_LSBZM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d90e684",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_concat_M_3D = the_2dto3d(X_train_NewF_M)\n",
    "del X_train_NewF_M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878dc7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_concat_Z_3D = the_2dto3d(X_train_NewF_Z)\n",
    "del X_train_NewF_Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2095b892",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_concat_M_N_3D = the_2dto3d(X_train_NewF_M_N)\n",
    "del X_train_NewF_M_N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8a34b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_concat_Z_N_3D = the_2dto3d(X_train_NewF_Z_N)\n",
    "del X_train_NewF_Z_N"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e814653",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbf7110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2768/2768 [==============================] - 16s 6ms/step\n",
      "(73492, 60, 24): LSTM Classifier is Done! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LSTM Concatenation\n",
    "lstm_concat = kfold_training('LSTM', X_train_concat_LSBZM_3D, Y_train_NewF_LSBZM, lstm_model, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556a91a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(lstm_concat, \"LSTM_Concatenation_Results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cf0bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Concatenation\n",
    "lstm_concat = kfold_training('LSTM', X_train_concat_M_3D, Y_train_NewF_M, lstm_model, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4678702",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(lstm_concat, \"LSTM_Concatenation_MM_Results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aeb5548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Concatenation\n",
    "lstm_concat = kfold_training('LSTM', X_train_concat_Z_3D, Y_train_NewF_Z, lstm_model, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e83968",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(lstm_concat, \"LSTM_Concatenation_ZM_Results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0426bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Concatenation\n",
    "lstm_concat = kfold_training('LSTM', X_train_concat_M_N_3D, Y_train_NewF_M_N, lstm_model, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39413204",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(lstm_concat, \"LSTM_Concatenation_MN_Results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3a5177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Concatenation\n",
    "lstm_concat = kfold_training('LSTM', X_train_concat_Z_N_3D, Y_train_NewF_Z_N, lstm_model, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2613d565",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(lstm_concat, \"LSTM_Concatenation_ZN_Results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba990f3",
   "metadata": {},
   "source": [
    "### 1D-CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bd7e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_concat = kfold_training('1DCNN', X_train_concat_LSBZM_3D, Y_train_NewF_LSBZM, cnn_model, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b80bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(cnn_concat, \"1DCNN_Concatenation_Results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c368b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_concat = kfold_training('1DCNN', X_train_concat_M_3D, Y_train_NewF_M, cnn_model, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef12e981",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(cnn_concat, \"1DCNN_Concatenation_MM_Results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98990092",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_concat = kfold_training('1DCNN', X_train_concat_Z_3D, Y_train_NewF_Z, cnn_model, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b078372",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(cnn_concat, \"1DCNN_Concatenation_ZM_Results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84856f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_concat = kfold_training('1DCNN', X_train_concat_M_N_3D, Y_train_NewF_M_N, cnn_model, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e9ad82",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(cnn_concat, \"1DCNN_Concatenation_MN_Results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bf5a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_concat = kfold_training('1DCNN', X_train_concat_Z_N_3D, Y_train_NewF_Z_N, cnn_model, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7276195",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(cnn_concat, \"1DCNN_Concatenation_ZN_Results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138f95dc",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d1964f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_concat = kfold_training('RNN', X_train_concat_LSBZM_3D, Y_train_NewF_LSBZM, rnn_model, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655326fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(rnn_concat, \"RNN_Concatenation_Results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1062b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_concat = kfold_training('RNN', X_train_concat_M_3D, Y_train_NewF_M, rnn_model, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a14d19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(rnn_concat, \"RNN_Concatenation_MM_Results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34b3237",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_concat = kfold_training('RNN', X_train_concat_Z_3D, Y_train_NewF_Z, rnn_model, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c482083",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(rnn_concat, \"RNN_Concatenation_ZM_Results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a97743",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_concat = kfold_training('RNN', X_train_concat_M_N_3D, Y_train_NewF_M_N, rnn_model, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ad8a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(rnn_concat, \"RNN_Concatenation_MN_Results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e12cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_concat = kfold_training('RNN', X_train_concat_Z_N_3D, Y_train_NewF_Z_N, rnn_model, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b58a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(rnn_concat, \"RNN_Concatenation_ZN_Results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d892b0",
   "metadata": {},
   "source": [
    "### GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a24d126",
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_concat = kfold_training('GRU', X_train_concat_LSBZM_3D, Y_train_NewF_LSBZM, gru_model, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf3258e",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(gru_concat, \"GRU_Concatenation_Results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3084410",
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_concat = kfold_training('GRU', X_train_concat_M_3D, Y_train_NewF_M, gru_model, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f963f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(gru_concat, \"GRU_Concatenation_MM_Results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf56b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_concat = kfold_training('GRU', X_train_concat_Z_3D, Y_train_NewF_Z, gru_model, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e871fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(gru_concat, \"GRU_Concatenation_ZM_Results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b00822",
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_concat = kfold_training('GRU', X_train_concat_M_N_3D, Y_train_NewF_M_N, gru_model, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6a1010",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(gru_concat, \"GRU_Concatenation_MN_Results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a946d337",
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_concat = kfold_training('GRU', X_train_concat_Z_N_3D, Y_train_NewF_Z_N, gru_model, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1591069d",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(gru_concat, \"GRU_Concatenation_ZN_Results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c600f8e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
